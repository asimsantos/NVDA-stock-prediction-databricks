{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6e1fd2-8280-4cb2-831c-b78a7bb02cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install yfinance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fbb2e1c-7c5b-4daa-8638-4e5b85b249bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Downloading and Previewing yfinance data table for $NVDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30cf0e1f-8034-43ed-ab52-f5647db50b96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "ticker = \"NVDA\"\n",
    "end = datetime.today()\n",
    "start = end - timedelta(days=365 * 10)  # ~10 years\n",
    "\n",
    "data = yf.download(ticker, start=start, end=end)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6245b41a-770e-4ea9-9c67-2d3a66cd86f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a new Unity Catalogue Volume to store the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5910d720-891c-416c-8046-929244502b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use Unity Catalog's default catalog \"main\"\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS main\")  # might already exist, that's fine\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS main.stocks\")\n",
    "spark.sql(\"USE CATALOG main\")\n",
    "spark.sql(\"USE SCHEMA stocks\")\n",
    "\n",
    "# Create a managed volume to hold the raw CSV files\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS main.stocks.nvda_raw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15b068c7-84e0-402d-97a7-301b30af659a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Store the yFinance Data in UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d6af61-dcd3-4e20-b83a-451e1a2db169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_path = \"/Volumes/main/stocks/nvda_raw\"  # folder for Bronze source files\n",
    "# If yfinance gave multi-index columns (e.g. ('Open', 'NVDA')), flatten them\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    data.columns = [\n",
    "        \"_\".join([str(c) for c in col if c != \"\"])\n",
    "        for col in data.columns\n",
    "    ]\n",
    "\n",
    "# Reset index so we have a date column\n",
    "data = data.reset_index()\n",
    "\n",
    "cols_lower = {str(c).lower(): c for c in data.columns}\n",
    "\n",
    "def get_col(possible_substrings, default=None):\n",
    "    for key_lower, orig in cols_lower.items():\n",
    "        for s in possible_substrings:\n",
    "            if s in key_lower:\n",
    "                return orig\n",
    "    return default\n",
    "\n",
    "date_col      = get_col([\"date\"])\n",
    "open_col      = get_col([\"open\"])\n",
    "high_col      = get_col([\"high\"])\n",
    "low_col       = get_col([\"low\"])\n",
    "close_col     = get_col([\"close\"])\n",
    "adj_close_col = get_col([\"adj close\", \"adj_close\", \"adjclose\"])\n",
    "volume_col    = get_col([\"volume\"])\n",
    "\n",
    "# 4) Build a clean DataFrame with standard column names\n",
    "data_clean = pd.DataFrame()\n",
    "data_clean[\"date\"]   = data[date_col]\n",
    "data_clean[\"open\"]   = data[open_col]\n",
    "data_clean[\"high\"]   = data[high_col]\n",
    "data_clean[\"low\"]    = data[low_col]\n",
    "data_clean[\"close\"]  = data[close_col]\n",
    "\n",
    "# If there is no explicit adj close column, fall back to close\n",
    "if adj_close_col is not None:\n",
    "    data_clean[\"adj_close\"] = data[adj_close_col]\n",
    "else:\n",
    "    data_clean[\"adj_close\"] = data[close_col]\n",
    "\n",
    "data_clean[\"volume\"] = data[volume_col]\n",
    "\n",
    "# 5) Write clean CSVs into the UC volume\n",
    "raw_path = \"/Volumes/main/stocks/nvda_raw\"\n",
    "\n",
    "spark_df = spark.createDataFrame(data_clean)\n",
    "\n",
    "(\n",
    "    spark_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")      # overwrite for now\n",
    "    .option(\"header\", True)\n",
    "    .csv(raw_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "101ac260-e2fd-490c-8e03-c1cbedbccccd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763017167204}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(raw_path)\n",
    "    .limit(5)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_nvda",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
