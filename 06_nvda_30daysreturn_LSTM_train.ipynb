{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd7fa9d-f6db-4bb4-a57d-6741e3707cec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install tensorflow scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "253fe812-1847-434e-8eef-a5349ad68ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load gold features\n",
    "df = spark.table(\"main.stocks.nvda_gold_features\").orderBy(\"date\")\n",
    "\n",
    "pdf = df.select(\n",
    "    \"date\",\n",
    "    \"adj_close\",\n",
    "    \"daily_return\",\n",
    "    \"ret_5d\",\n",
    "    \"ret_20d\",\n",
    "    \"vol_20d\",\n",
    "    \"fwd_30d_return\"\n",
    ").toPandas().dropna()\n",
    "\n",
    "pdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0937cb08-3c21-4120-a6e3-0fed910e4a3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_cols = [\"adj_close\", \"daily_return\", \"ret_5d\", \"ret_20d\", \"vol_20d\"]\n",
    "target_col  = \"fwd_30d_return\"\n",
    "\n",
    "lookback = 60  # use last 60 days to predict 30-day forward return\n",
    "\n",
    "values = pdf[feature_cols].values\n",
    "targets = pdf[target_col].values\n",
    "dates   = pd.to_datetime(pdf[\"date\"]).values\n",
    "\n",
    "X_list, y_list, date_list = [], [], []\n",
    "\n",
    "for i in range(lookback, len(pdf)):\n",
    "    # past 60 days of features\n",
    "    X_list.append(values[i - lookback:i, :])\n",
    "    # 30-day forward return for \"today\"\n",
    "    y_list.append(targets[i])\n",
    "    # label date (for plotting later)\n",
    "    date_list.append(dates[i])\n",
    "\n",
    "X = np.array(X_list)      # (samples, timesteps, features)\n",
    "y = np.array(y_list)      # (samples,)\n",
    "date_arr = np.array(date_list)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5783ef46-c927-4f19-be7c-4d485e248310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "split_idx = int(len(X) * 0.8)\n",
    "\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "dates_test = date_arr[split_idx:]\n",
    "\n",
    "len(X_train), len(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad18781-960e-4e60-8ca1-cc288754e5ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "n_timesteps = X.shape[1]\n",
    "n_features  = X.shape[2]\n",
    "\n",
    "model_lstm_30d = Sequential()\n",
    "model_lstm_30d.add(LSTM(64, input_shape=(n_timesteps, n_features)))\n",
    "model_lstm_30d.add(Dropout(0.2))\n",
    "model_lstm_30d.add(Dense(1))  # regression: 30-day forward return (%)\n",
    "\n",
    "model_lstm_30d.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "history = model_lstm_30d.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5e808ee-84f8-4d08-b3f1-986ab9ce79e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test = model_lstm_30d.predict(X_test).flatten()\n",
    "\n",
    "rmse = np.sqrt(\n",
    "    mean_squared_error(\n",
    "        y_test,\n",
    "        y_pred_test\n",
    "    )\n",
    ")\n",
    "print(\"LSTM 30-day return RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571b0693-f7f9-4425-921a-bfd871c12940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predictions for ALL sequences (entire history after first lookback)\n",
    "y_pred_all = model_lstm_30d.predict(X).flatten()\n",
    "\n",
    "pred_lstm_30d_pdf = pd.DataFrame()\n",
    "pred_lstm_30d_pdf[\"date\"] = pd.to_datetime(date_arr).astype(\"datetime64[ns]\").date\n",
    "pred_lstm_30d_pdf[\"actual_fwd_30d_return\"] = y\n",
    "pred_lstm_30d_pdf[\"pred_fwd_30d_return_lstm\"] = y_pred_all\n",
    "\n",
    "pred_lstm_30d_pdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca4d2fb-994a-41a6-af36-8942a466997e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pred_lstm_30d_spark = spark.createDataFrame(pred_lstm_30d_pdf)\n",
    "\n",
    "pred_lstm_30d_spark.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"main.stocks.nvda_predictions_lstm_30d_return\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_nvda_30daysreturn_LSTM_train",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
